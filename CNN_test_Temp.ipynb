{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code follows the architecture laid down in Ba√±o-Medina et al.,2020 for CNN10 architecture for temperature downscaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function:  standardising the input temperature data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_temperature_data(X):\n",
    "\n",
    "    scaler= StandardScaler() #Ensures all input variables have the same scale\n",
    "    X_scaled= scaler.fit_transform(X.reshape(-1, X.shape[-1]).reshape(X.shape))\n",
    "    return X_scaled, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Temperature Downscaling CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempDownscalingCNN(nn.Module):\n",
    "    '''Defines a Pytorch neural network model for temperature downscaling based on number of input predictors variables (5 in this case on four different pressure levels)\n",
    "    and the output size (1 in this case, corr to one daily temperature value at each grid point)'''\n",
    "    def __init__(self,input_channels,output_size):\n",
    "        super(TempDownscalingCNN,self).__init__()\n",
    "\n",
    "        #The convolutional layers : There are three in this architecture following CNN10\n",
    "        self.conv1= nn.Conv2d(input_channels, 50, kernel_size=3) #50 is the number of kernels, 3x3x20 (20 =5 variables, 4 pressure levels)\n",
    "        self.conv2= nn.Conv2d(50,25,kernel_size=3) #Dimensions of the kernel are 3x3x50, based on the output from the last layer\n",
    "        self.conv3= nn.Conv2d(25,10,kernel_size=3) #3x3x25, as the previous layer had 25 kernels\n",
    "\n",
    "        #So the last convo layer above has 10 kernels of size 3x3. The output from this layer is then flattened and fed into the fully connected layers\n",
    "        #The fully connected layers \n",
    "        self.flatten= nn.Flatten() #The output from the last convo layer is flattened to be fed into the dense layer. Fully connected dense layers in \n",
    "        #Pytorch are implemented as Linear Layers (expect a 1 D input)\n",
    "    \n",
    "        #Now the dense layers. \n",
    "        \n",
    "        self.fc1= nn.Linear(10 *(output_size-6)* (output_size-6) ,output_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(10 * (output_size-6) * (output_size-6), output_size)\n",
    "\n",
    "    #Now, defining the forward pass\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''Defining the forward pass of the Temperature Downscaling Model'''\n",
    "        x=F.relu(self.conv1(x)) #Applying first convolution and activation\n",
    "        x=F.relu(self.conv2(x)) #Applying second convolution and activation\n",
    "        x=F.relu(self.conv3(x)) #Applying the third convolution and activation\n",
    "        x=self.flatten(x) #Flattening the output from the last convolutional layer\n",
    "        l51=self.fc1(x) #Applying the first fully connected layer\n",
    "        l52=self.fc2(x) #Applying the second fully connected layer\n",
    "        \n",
    "        \n",
    "        return torch.cat((l51,l52),dim=1)  #Concatenating the output from the last two fully connected layers, in case of temperature downscaling\n",
    "    \n",
    "    #Defining the loss function\n",
    "    \n",
    "    def loss (pred,target):\n",
    "        '''Defining the loss function for the model. In this case, the MSE loss is used, also called Gaussian loss, suitable for temperature'''\n",
    "\n",
    "        mean_pred= pred[:, :target.size(1)] #The first half of the output is the mean\n",
    "\n",
    "        return F.mse_loss(mean_pred,target)\n",
    "    \n",
    "\n",
    "    #Writing the training loop\n",
    "\n",
    "    def train_model(model, dataloader, criterion, optimizer, num_epochs=10000, device=device, patience=30):\n",
    "        #Patience : to prevent overfitting in the model, stops the training if loss doesnt improve after 30 epochs\n",
    "        '''Function to train the temperature downscaling model'''\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        #The training loop\n",
    "        best_loss=float('inf')\n",
    "        patience_counter=0\n",
    "\n",
    "\n",
    "        for epoch in range (num_epochs):\n",
    "            model.train()\n",
    "            running_loss=0.0\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs, targets=inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss=criterion(outputs,targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss+=loss.item()*input_size(0)\n",
    "\n",
    "            epoch_loss= running_loss/len(dataloader.dataset)\n",
    "\n",
    "#Implementing early stopping to prevent overfitting\n",
    "            if epoch_loss< best_loss:\n",
    "                best_loss=epoch_loss\n",
    "                patience_counter=0\n",
    "\n",
    "                #Saving the best model\n",
    "                torch.save(model.state_dict(), 'best_model_temp_downscaling.pth')\n",
    "\n",
    "            else:\n",
    "                patience_counter+=1\n",
    "\n",
    "            if patience_counter>patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} with best loss {best_loss}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model,dataloader):\n",
    "    '''Function to make downscaled predictions using the trained model'''\n",
    "    model.eval()\n",
    "\n",
    "    predictions=[]\n",
    "    with torch.no_grad():\n",
    "        for inputs,targets in dataloader:\n",
    "            outputs=model(inputs.to(device))\n",
    "            mean_pred=outputs[:,:inputs.size(1)]\n",
    "            predictions.append(mean_pred.cpu())\n",
    "    return torch.cat(predictions,dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Training the model and validating within the historical period on 20 percent of the data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_scaled, scaler \u001b[38;5;241m=\u001b[39m standardise_temperature_data(\u001b[43mX\u001b[49m) \u001b[38;5;66;03m# Using the standardisation function to normalise the input data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X_train,X_val,y_train,y_val\u001b[38;5;241m=\u001b[39mtrain_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \u001b[38;5;66;03m#Doing a 90:10 training :validation split\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Converting the input data into pytorch tensors\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#Training the model and validating within the historical period on 20 percent of the data\n",
    "\n",
    "X_scaled, scaler = standardise_temperature_data(X) # Using the standardisation function to normalise the input data\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_scaled, y, test_size=0.1,random_state=42) #Doing a 90:10 training :validation split\n",
    "\n",
    "#Converting the input data into pytorch tensors\n",
    "\n",
    "train_data= TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_data= TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val,dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputting the data via the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_channels\u001b[38;5;241m=\u001b[39m\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;66;03m#Number of input variables , geopotential height, zonal wind, meridional wind, specific humidity and temperature\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output_size\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Temperature at each grid point for the entire spatial domain\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "input_channels=X.shape[5] #Number of input variables , geopotential height, zonal wind, meridional wind, specific humidity and temperature\n",
    "output_size= y.shape[1] # Temperature at each grid point for the entire spatial domain\n",
    "temperature_downscaling_model_V01= TempDownscalingCNN(input_channels,output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= optim.Adam(temperature_downscaling_model_V01.parameters(), lr=0.001)\n",
    "criterion= nn.MSELoss()\n",
    "\n",
    "train_model(temperature_downscaling_model_V01, train_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
