{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli Gamma Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBGLoss\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(BGLoss,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[40], line 22\u001b[0m, in \u001b[0;36mBGLoss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#Creating a binary mask for precipitation : 1 if precip exceeds 0, 0 otherwise , denoting occurence of precipitation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m occurence_mask\u001b[38;5;241m=\u001b[39m(y_train\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 22\u001b[0m bernoulli_loss\u001b[38;5;241m=\u001b[39m  occurence_mask\u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(p_pred\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-10\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39moccurence_mask)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mp_pred\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\n\u001b[1;32m     23\u001b[0m bernoulli_loss\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(bernoulli_loss)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#Computing the Gamma loss\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "class BGLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BGLoss,self).__init__()\n",
    "\n",
    "    def forward(self, p_pred, log_alpha_pred, log_beta_pred,y_train):\n",
    "\n",
    "            '''Computes the BG loss for the given batch for the purposes of downscaling precipitation\n",
    "            Parameters:\n",
    "            p_pred: Predicted probability of precipitation\n",
    "            log_alpha_pred: Predicted parameter for Gamma Distribution\n",
    "            log_beta_pred: Predicted parameter for Gamma Distribution\n",
    "            y_true: True Precip value\n",
    "            \n",
    "            The function returns the total loss combining the Bernoulli and Gamma losses'''\n",
    "\n",
    "            #Creating a binary mask for precipitation : 1 if precip exceeds 0, 0 otherwise , denoting occurence of precipitation\n",
    "\n",
    "    occurence_mask=(y_train>0).float()\n",
    "\n",
    "\n",
    "    bernoulli_loss=  occurence_mask* torch.log(p_pred+1e-10) + (1-occurence_mask)*torch.log(1-p_pred+1e-10)\n",
    "    bernoulli_loss=torch.mean(bernoulli_loss)\n",
    "\n",
    "\n",
    "        #Computing the Gamma loss\n",
    "\n",
    "    if occurence_mask.sum()>0:\n",
    "            alpha= torch.exp(log_alpha_pred)\n",
    "            beta=torch.exp(log_beta_pred)\n",
    "\n",
    "                #Negative log likelihood\n",
    "            gamma_loss= alpha*torch.log(beta+1e-10) -torch.lgamma(alpha) + (alpha-1)*torch.log(y_train+1e-10) - beta*y_train\n",
    "            gamma_loss=-gamma_loss\n",
    "            gamma_loss=gamma_loss*occurence_mask.sum()/occurence_mask.sum() #Normalising over the raining points\n",
    "\n",
    "    else:\n",
    "            gamma_loss=torch.tensor(0.0,device=y_train.device)\n",
    "\n",
    "total_loss=bernoulli_loss+gamma_loss\n",
    "return total_loss  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model for Downscaling Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model for Downscaling Precipitation\n",
    "class PrecipDownscalingCNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(PrecipDownscalingCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with SAME padding\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=25, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=25, out_channels=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Flatten the feature maps\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers for precipitation prediction\n",
    "        flattened_size = output_size * output_size  # Assuming input has spatial dimensions preserved\n",
    "        self.fc_p = nn.Linear(flattened_size, output_size)  # Precipitation occurrence (sigmoid)\n",
    "        self.fc_log_alpha = nn.Linear(flattened_size, output_size)  # Gamma distribution parameter\n",
    "        self.fc_log_beta = nn.Linear(flattened_size, output_size)  # Gamma distribution parameter\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Three outputs\n",
    "        p_pred = torch.sigmoid(self.fc_p(x))  # Probability of precipitation\n",
    "        log_alpha = self.fc_log_alpha(x)  # Gamma parameter\n",
    "        log_beta = self.fc_log_beta(x)  # Gamma parameter\n",
    "        \n",
    "        return torch.cat((p_pred, log_alpha, log_beta), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale_Data(x):\n",
    "    '''Standadizes Input Precip Data: mean 0 unit variance'''\n",
    "    mean= x.mean()\n",
    "    std=x.std()\n",
    "    return (x-mean)/(std +1e-10), mean, std\n",
    "\n",
    "def binary_precipitation(y):\n",
    "    '''0 if it didnt rain, 1 if it did'''\n",
    "\n",
    "    return(y>0.99).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 100, 100)  # Assuming 32x32 input...will be changed later\n",
    "output_size=100 # Assuming 32x32 output...will be changed later\n",
    "lr=0.0001\n",
    "batch_size=100\n",
    "epochs=10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example data for training\n",
    "x_train=torch.randn(100,1,32,32)\n",
    "y_train=torch.randn(100,1,32,32)\n",
    "y_train=binary_precipitation(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x1024 and 10000x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model_precip_V0\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_precip_V0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Predicted values\u001b[39;00m\n\u001b[1;32m     12\u001b[0m p_pred\u001b[38;5;241m=\u001b[39mpred[:, :y_train\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mPrecipDownscalingCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Three outputs\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m p_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Probability of precipitation\u001b[39;00m\n\u001b[1;32m     28\u001b[0m log_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_log_alpha(x)  \u001b[38;5;66;03m# Gamma parameter\u001b[39;00m\n\u001b[1;32m     29\u001b[0m log_beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_log_beta(x)  \u001b[38;5;66;03m# Gamma parameter\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x1024 and 10000x100)"
     ]
    }
   ],
   "source": [
    "model_precip_V0=PrecipDownscalingCNN(input_shape,output_size)\n",
    "loss_fn=BGLoss() #Bernoulli Gamma Loss, defined and created before\n",
    "optimizer=optim.Adam(model_precip_V0.parameters(),lr=lr)\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_precip_V0.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred=model_precip_V0(x_train)\n",
    "\n",
    "    #Predicted values\n",
    "    p_pred=pred[:, :y_train.size(1)]\n",
    "    log_alpha_pred= pred[:, y_train.size(1):2*y_train.size(1)]\n",
    "    log_beta_pred= pred[:,2*y_train.size(1):]\n",
    "\n",
    "    loss_fn=BGLoss()\n",
    "    #Computing the loss\n",
    "    loss= loss_fn(p_pred, log_alpha_pred, log_beta_pred, y_train)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model prediction\n",
    "def predict_precipitation(model,x):\n",
    "    '''Predicts precipitation for a given set of inputs'''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred=model(x)\n",
    "        p_pred(pred[:, :y_train.size(1)])\n",
    "        log_alpha_pred(pred[:, y_train.size(1):2*y_train.size(1)])\n",
    "        log_beta_pred(pred[:,2*y_train.size(1):]) \n",
    "\n",
    "        return p_pred,log_alpha_pred,log_beta_pred          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving predictions\n",
    "def save_predictions(model,x,y):\n",
    "    '''Saves the predictions for a given set of inputs'''\n",
    "\n",
    "    ds=xr.Dataset({'precip':(['time','lat','lon'],y),\n",
    "                   'p_pred':(['time','lat','lon'],p_pred)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the shape of the tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pred: torch.Size([100, 96])\n",
      "Shape of p_pred: torch.Size([100, 32])\n",
      "Shape of log_alpha_pred: torch.Size([100, 32])\n",
      "Shape of log_beta_pred: torch.Size([100, 32])\n",
      "Shape of y_train: torch.Size([100, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of pred:\", pred.shape)\n",
    "print(\"Shape of p_pred:\", p_pred.shape)\n",
    "print(\"Shape of log_alpha_pred:\", log_alpha_pred.shape)\n",
    "print(\"Shape of log_beta_pred:\", log_beta_pred.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
