experiment:
  batch_size: 32
  num_workers: 4
  quick_test: false

variables:
  input:
    precip: pr
    temp: tas
  target:
    precip: RhiresD
    temp: TabsD

preprocessing:
  nan_to_num: true
  nan_value : 0.0

split:
  method: "by_decade"
  seed: 42
  train_frac: 0.7
  val_frac: 0.2
  #Above code Will be later customised for spatial cross val ewtcc

train:
  num_epochs: 20
  checkpoint_path: ../checkpoints/best_model_Triangular_FULL.pth
  in_channels: 2
  out_channels: 2
  optimizer: "Adam"
  loss_fn: "MSE"
  scheduler: "CyclicLR"
  #scheduler: "ReduceLROnPlateau"
  #scheduler_mode: "min"
  scheduler_mode: "triangular" #Can be changed to triangular2 or triangular depending on requirements
  base_lr: 1e-4
  max_lr: 1e-3
  #scheduler_factor: 0.5 #Halving the lr upon plateauing
  #scheduler_patience: 3 #Waiting for N epochs for no improvement
  #How was step size chosen? Numer of iterations(training batches) per epoch= len(training_set)/batch_size
  #So following from above, step size up= Number iter per epoch/2
  step_size_up: 208
  
  wandb_project: "UNet_Deterministic_FULL_training"
  wandb_run_name: "Experiment_02_Triangular"

# Placeholder for the paths in the untracked .paths.yaml file
data: {}
